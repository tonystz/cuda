{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonystz/gitpod/blob/main/strings_To_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycuda # install cuda\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule"
      ],
      "metadata": {
        "id": "6QAStcPlRxWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "ttt = np.asarray([ \"stuff\" + str(i)  for i in range(0,2) ])\n",
        "\n",
        "print( ttt.dtype, type(ttt[0]) ) \n",
        "print(ttt)\n",
        "\n",
        "s= np.array(['abcdf'], dtype=object)\n",
        "print( s.dtype, type(s[0]) )\n",
        "\n",
        "#print(\"ctype:\",np.ctypeslib.as_ctypes_type(s.dtype))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIfpULJwlMWY",
        "outputId": "b5f001a6-78bc-4406-f493-b7650a4b6f3a"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<U6 <class 'numpy.str_'>\n",
            "['stuff0' 'stuff1']\n",
            "object <class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## all string size must equal?"
      ],
      "metadata": {
        "id": "cWZj7ByhRDsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile s.py\n",
        "#!python \n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "from pycuda import gpuarray\n",
        "import numpy as np\n",
        "\n",
        "mod = SourceModule(\"\"\"\n",
        "    #include <stdio.h>\n",
        "\n",
        "    __global__ void say_hi(char*out_gpu, unsigned int *s_offset_gpu)\n",
        "    { \n",
        "      int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "      //printf(\"index: threadIdx.x=%d blockIdx.x=%d  blockDim.x=%d \\\\n\",threadIdx.x,blockIdx.x,blockDim.x);\n",
        "      //printf(\"thread id: [%d]  offset:%d \\\\n\",idx,s_offset_gpu[idx]);\n",
        "      printf(\"thread id: [%d]  offset:%d  get str:%s\\\\n\",idx,s_offset_gpu[idx],out_gpu+s_offset_gpu[idx]);\n",
        "    }\n",
        "    \"\"\")\n",
        "\n",
        "say_hi = mod.get_function(\"say_hi\")\n",
        "\n",
        "\n",
        "s= np.array(['abcm','edfg','s12w','h45q'], dtype=np.string_)\n",
        "\n",
        "offset=[0]\n",
        "for i in s:\n",
        "  offset.append(offset[-1]+len(i))\n",
        "s_offset=np.array(offset,dtype=np.uint32)\n",
        "print('shape:',s.shape)\n",
        "print(s,s.data)\n",
        "print('offset:',s_offset,s_offset.shape)\n",
        "\n",
        "s_gpu = gpuarray.to_gpu(s)\n",
        "s_offset_gpu = gpuarray.to_gpu(s_offset)\n",
        "\n",
        "say_hi(s_gpu,s_offset_gpu,block=(s.size,1,1),grid=(1,1,1))\n",
        "print('modify:',s_gpu.get(),s_offset_gpu.get())\n",
        "\n",
        "print(\"s_offset ctype:\",np.ctypeslib.as_ctypes_type(s_offset.dtype))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3vCpSIuULQt",
        "outputId": "8fd16c80-d4ec-479c-ba37-e3a4d1e6fed9"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting s.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python s.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNIVqz_mUS2u",
        "outputId": "20e2724b-5a4d-405f-9cd8-a56896513cfe"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (4,)\n",
            "[b'abcm' b'edfg' b's12w' b'h45q'] <memory at 0x7f35263b0580>\n",
            "offset: [ 0  4  8 12 16] (5,)\n",
            "thread id: [0]  offset:0  get str:abcmedfgs12wh45q\n",
            "thread id: [1]  offset:4  get str:edfgs12wh45q\n",
            "thread id: [2]  offset:8  get str:s12wh45q\n",
            "thread id: [3]  offset:12  get str:h45q\n",
            "modify: [b'abcm' b'edfg' b's12w' b'h45q'] [ 0  4  8 12 16]\n",
            "s_offset ctype: <class 'ctypes.c_uint'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test un-equal string process?"
      ],
      "metadata": {
        "id": "q0aII8SvieuC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TaQ8-EtoieRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile t389.cu\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "\n",
        "#define nTPB 256\n",
        "\n",
        "__global__ void kern_1D(char *data, unsigned *indices, unsigned num_strings){\n",
        "\n",
        "  int idx = threadIdx.x+blockDim.x*blockIdx.x;\n",
        "  if (idx < num_strings)\n",
        "    printf(\"Hello from thread %d, offset=%d, my string is %s\\n\", idx, indices[idx],data+indices[idx]);\n",
        "}\n",
        "\n",
        "__global__ void kern_2D(char **data, unsigned num_strings){\n",
        "\n",
        "  int idx = threadIdx.x+blockDim.x*blockIdx.x;\n",
        "  if (idx < num_strings)\n",
        "    printf(\"Hello from thread %d, my string is %s\\n\", idx, data[idx]);\n",
        "}\n",
        "\n",
        "int main(){\n",
        "\n",
        "  const int num_strings = 3;\n",
        "  const char s0[] = \"s1\\0\";\n",
        "  const char s1[] = \"s2\\0\";\n",
        "  const char s2[] = \"sstz3\";\n",
        "  int ds[num_strings];\n",
        "  ds[0] = sizeof(s0)/sizeof(char);\n",
        "  ds[1] = sizeof(s1)/sizeof(char);\n",
        "  ds[2] = sizeof(s2)/sizeof(char);\n",
        "  // pretend we have a dynamically allocated char** array\n",
        "  char **data;\n",
        "  data = (char **)malloc(num_strings*sizeof(char *));\n",
        "  data[0] = (char *)malloc(ds[0]*sizeof(char));\n",
        "  data[1] = (char *)malloc(ds[1]*sizeof(char));\n",
        "  data[2] = (char *)malloc(ds[2]*sizeof(char));\n",
        "  // initialize said array\n",
        "  strcpy(data[0], s0);\n",
        "  strcpy(data[1], s1);\n",
        "  strcpy(data[2], s2);\n",
        "  // method 1: \"flattening\"\n",
        "  char *fdata = (char *)malloc((ds[0]+ds[1]+ds[2])*sizeof(char));\n",
        "  unsigned *ind   = (unsigned *)malloc(num_strings*sizeof(unsigned));\n",
        "  unsigned next = 0;\n",
        "  for (int i = 0; i < num_strings; i++){\n",
        "    strcpy(fdata+next, data[i]);\n",
        "    ind[i] = next;\n",
        "    next += ds[i];}\n",
        "  //copy to device\n",
        "  char *d_fdata;\n",
        "  unsigned *d_ind;\n",
        "  cudaMalloc(&d_fdata, next*sizeof(char));\n",
        "  cudaMalloc(&d_ind, num_strings*sizeof(unsigned));\n",
        "  cudaMemcpy(d_fdata, fdata, next*sizeof(char), cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_ind, ind, num_strings*sizeof(unsigned), cudaMemcpyHostToDevice);\n",
        "  printf(\"method 1: %d  --> %d\\n\",(num_strings+nTPB-1)/nTPB,nTPB);\n",
        "  kern_1D<<<(num_strings+nTPB-1)/nTPB, nTPB>>>(d_fdata, d_ind, num_strings);\n",
        "  cudaDeviceSynchronize();\n",
        "  //method 2: \"2D\" (pointer-to-pointer) array\n",
        "  char **d_data;\n",
        "  cudaMalloc(&d_data, num_strings*sizeof(char *));\n",
        "  char **d_temp_data;\n",
        "  d_temp_data = (char **)malloc(num_strings*sizeof(char *));\n",
        "  for (int i = 0; i < num_strings; i++){\n",
        "    cudaMalloc(&(d_temp_data[i]), ds[i]*sizeof(char));\n",
        "    cudaMemcpy(d_temp_data[i], data[i], ds[i]*sizeof(char), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_data+i, &(d_temp_data[i]), sizeof(char *), cudaMemcpyHostToDevice);}\n",
        "  printf(\"method 2:\\n\");\n",
        "  kern_2D<<<(num_strings+nTPB-1)/nTPB, nTPB>>>(d_data, num_strings);\n",
        "  cudaDeviceSynchronize();\n",
        "  // method 3: managed allocations\n",
        "  // start over with a managed char** array\n",
        "  char **m_data;\n",
        "  cudaMallocManaged(&m_data, num_strings*sizeof(char *));\n",
        "  cudaMallocManaged(&(m_data[0]), ds[0]*sizeof(char));\n",
        "  cudaMallocManaged(&(m_data[1]), ds[1]*sizeof(char));\n",
        "  cudaMallocManaged(&(m_data[2]), ds[2]*sizeof(char));\n",
        "  // initialize said array\n",
        "  strcpy(m_data[0], s0);\n",
        "  strcpy(m_data[1], s1);\n",
        "  strcpy(m_data[2], s2);\n",
        "  // call kernel directly on managed data\n",
        "  printf(\"method 3:\\n\");\n",
        "  kern_2D<<<(num_strings+nTPB-1)/nTPB, nTPB>>>(m_data, num_strings);\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LixIpFBrd7x",
        "outputId": "539ec028-99ba-46bc-a6a7-cc1c848659e6"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing t389.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o t389 t389.cu\n",
        "!compute-sanitizer ./t389"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sSHVTI7rrBI",
        "outputId": "e986e3f5-1bd4-49e5-9861-8cf1bfa625a5"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========= COMPUTE-SANITIZER\n",
            "method 1: 1  --> 256\n",
            "Hello from thread 0, offset=0, my string is s1\n",
            "Hello from thread 1, offset=4, my string is s2\n",
            "Hello from thread 2, offset=8, my string is sstz3\n",
            "method 2:\n",
            "Hello from thread 0, my string is s1\n",
            "Hello from thread 1, my string is s2\n",
            "Hello from thread 2, my string is sstz3\n",
            "method 3:\n",
            "Hello from thread 0, my string is s1\n",
            "Hello from thread 1, my string is s2\n",
            "Hello from thread 2, my string is sstz3\n",
            "========= ERROR SUMMARY: 0 errors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile s2.py\n",
        "import time\n",
        "import numpy as np\n",
        "from pycuda import driver, compiler, gpuarray, tools\n",
        "import math\n",
        "from sys import getsizeof\n",
        "\n",
        "import pycuda.autoinit\n",
        "\n",
        "kernel_code1 = \"\"\"\n",
        "__global__ void test1(char** d_wordList) {\n",
        "    (d_wordList[blockIdx.x][threadIdx.x])++;      \n",
        "}\n",
        "    \"\"\"\n",
        "\n",
        "kernel_code2 = \"\"\"\n",
        "__global__ void test2(char* d_wordList, size_t *offsets) {\n",
        "\n",
        "    int idx = threadIdx.x+blockDim.x*blockIdx.x;\n",
        "    printf(\"index: threadIdx.x=%d blockIdx.x=%d  blockDim.x=%d \\\\n\",threadIdx.x,blockIdx.x,blockDim.x);\n",
        "    printf(\"Hello from thread %d, my string is %s ,offsets:%u \\\\n\", idx, d_wordList+offsets[idx],offsets[blockIdx.x]);\n",
        "    (d_wordList[offsets[blockIdx.x] + threadIdx.x])++;\n",
        "}\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "mod = compiler.SourceModule(kernel_code1)\n",
        "ker_test1 = mod.get_function(\"test1\")\n",
        "\n",
        "\n",
        "\n",
        "wordList = ['asd','bsd','csd']\n",
        "\n",
        "d_words = []\n",
        "\n",
        "for word in wordList:\n",
        "    d_words.append(gpuarray.to_gpu(np.array(word, dtype=str)))\n",
        "\n",
        "d_wordList = gpuarray.to_gpu(np.array([word.ptr for word in d_words], dtype=np.uintp))\n",
        "\n",
        "ker_test1(d_wordList, block=(3,1,1), grid=(3,1,1))\n",
        "\n",
        "for word in d_words:\n",
        "  result = word.get()\n",
        "  print(result)\n",
        "\n",
        "mod2 = compiler.SourceModule(kernel_code2)\n",
        "ker_test2 = mod2.get_function(\"test2\")\n",
        "\n",
        "d_words2 = gpuarray.to_gpu(np.array(['asd','bsd','csd'], dtype=np.string_))\n",
        "offsets = gpuarray.to_gpu(np.array([0,3,6,9], dtype=np.uint64))\n",
        "ker_test2(d_words2, offsets, block=(3,1,1), grid=(1,1,1))\n",
        "h_words2 = d_words2.get()\n",
        "print(h_words2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vo6DMw4Ozkaz",
        "outputId": "e1202927-a092-47cf-bd0e-ceac9e52727f"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting s2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python s2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZSN14p-z1rR",
        "outputId": "44dd273a-4a5c-4167-e164-97a6cc652361"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "𐅢sd\n",
            "𐅣sd\n",
            "𐅤sd\n",
            "index: threadIdx.x=0 blockIdx.x=0  blockDim.x=3 \n",
            "index: threadIdx.x=1 blockIdx.x=0  blockDim.x=3 \n",
            "index: threadIdx.x=2 blockIdx.x=0  blockDim.x=3 \n",
            "Hello from thread 0, my string is asdbsdcsd ,offsets:0 \n",
            "Hello from thread 1, my string is bsdcsd ,offsets:0 \n",
            "Hello from thread 2, my string is csd ,offsets:0 \n",
            "[b'bte' b'bsd' b'csd']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K7A24j3mRsTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oM8mVJsjRvSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tamlFvddB1KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### reference:\n",
        "https://stackoverflow.com/questions/34700999/cuda-passing-char-to-kernel/34712905"
      ],
      "metadata": {
        "id": "x_WJoSceKlMM"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}