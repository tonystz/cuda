{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonystz/gitpod/blob/main/test_pycuda_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2JZt1GL5D6W"
      },
      "source": [
        "# Introduction to CUDA and PyCUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FA_YN7HlGRP5"
      },
      "outputs": [],
      "source": [
        "!pip install pycuda # install cuda\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "O45Aq6Bzzcc-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ced442c7-b122-4aaa-da6f-094d2d32d6a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Mar 14 11:40:25 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P0    28W /  70W |    103MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pycuda\n",
        "import pycuda.driver as drv\n",
        "drv.init()\n",
        "\n",
        "print('CUDA device query (PyCUDA version) \\n')\n",
        "\n",
        "print('Detected {} CUDA Capable device(s) \\n'.format(drv.Device.count()))\n",
        "\n",
        "for i in range(drv.Device.count()):\n",
        "    \n",
        "    gpu_device = drv.Device(i)\n",
        "    print('Device {}: {}'.format( i, gpu_device.name() )) \n",
        "    compute_capability = float( '%d.%d' % gpu_device.compute_capability() )\n",
        "    print('\\t Compute Capability: {}'.format(compute_capability))\n",
        "    print('\\t Total Memory: {} megabytes'.format(gpu_device.total_memory()//(1024**2)))\n",
        "\n",
        "\n",
        "    # The following will give us all remaining device attributes as seen \n",
        "    # in the original deviceQuery.\n",
        "    # We set up a dictionary as such so that we can easily index\n",
        "    # the values using a string descriptor.\n",
        "    \n",
        "    device_attributes_tuples = iter(gpu_device.get_attributes().items()) \n",
        "    device_attributes = {}\n",
        "\n",
        "        \n",
        "    for k, v in device_attributes_tuples:\n",
        "        device_attributes[str(k)] = v\n",
        "        # print(f'{k}->{v}')\n",
        "    # continue\n",
        "    num_mp = device_attributes['MULTIPROCESSOR_COUNT']\n",
        "    \n",
        "    # Cores per multiprocessor is not reported by the GPU!  \n",
        "    # We must use a lookup table based on compute capability.\n",
        "    # See the following:\n",
        "    # http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities\n",
        "    \n",
        "    cuda_cores_per_mp = { 5.0 : 128, 5.1 : 128, 5.2 : 128, 6.0 : 64, 6.1 : 128, 7.5 : 128}[compute_capability]\n",
        "    \n",
        "    print('\\t ({}) Multiprocessors, ({}) CUDA Cores / Multiprocessor: {} CUDA Cores'.format(num_mp, cuda_cores_per_mp, num_mp*cuda_cores_per_mp))\n",
        "    \n",
        "    device_attributes.pop('MULTIPROCESSOR_COUNT')\n",
        "    \n",
        "    for k in list(device_attributes.keys()):\n",
        "        print('\\t {}: {}'.format(k, device_attributes[k]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkYg1h5iF-x6",
        "outputId": "c4d0876a-55f5-436f-94ca-b5fb179988be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA device query (PyCUDA version) \n",
            "\n",
            "Detected 1 CUDA Capable device(s) \n",
            "\n",
            "Device 0: Tesla T4\n",
            "\t Compute Capability: 7.5\n",
            "\t Total Memory: 15101 megabytes\n",
            "\t (40) Multiprocessors, (128) CUDA Cores / Multiprocessor: 5120 CUDA Cores\n",
            "\t ASYNC_ENGINE_COUNT: 3\n",
            "\t CAN_MAP_HOST_MEMORY: 1\n",
            "\t CAN_USE_HOST_POINTER_FOR_REGISTERED_MEM: 1\n",
            "\t CLOCK_RATE: 1590000\n",
            "\t COMPUTE_CAPABILITY_MAJOR: 7\n",
            "\t COMPUTE_CAPABILITY_MINOR: 5\n",
            "\t COMPUTE_MODE: DEFAULT\n",
            "\t COMPUTE_PREEMPTION_SUPPORTED: 1\n",
            "\t CONCURRENT_KERNELS: 1\n",
            "\t CONCURRENT_MANAGED_ACCESS: 1\n",
            "\t DIRECT_MANAGED_MEM_ACCESS_FROM_HOST: 0\n",
            "\t ECC_ENABLED: 1\n",
            "\t GENERIC_COMPRESSION_SUPPORTED: 0\n",
            "\t GLOBAL_L1_CACHE_SUPPORTED: 1\n",
            "\t GLOBAL_MEMORY_BUS_WIDTH: 256\n",
            "\t GPU_OVERLAP: 1\n",
            "\t HANDLE_TYPE_POSIX_FILE_DESCRIPTOR_SUPPORTED: 1\n",
            "\t HANDLE_TYPE_WIN32_HANDLE_SUPPORTED: 0\n",
            "\t HANDLE_TYPE_WIN32_KMT_HANDLE_SUPPORTED: 0\n",
            "\t HOST_NATIVE_ATOMIC_SUPPORTED: 0\n",
            "\t INTEGRATED: 0\n",
            "\t KERNEL_EXEC_TIMEOUT: 0\n",
            "\t L2_CACHE_SIZE: 4194304\n",
            "\t LOCAL_L1_CACHE_SUPPORTED: 1\n",
            "\t MANAGED_MEMORY: 1\n",
            "\t MAXIMUM_SURFACE1D_LAYERED_LAYERS: 2048\n",
            "\t MAXIMUM_SURFACE1D_LAYERED_WIDTH: 32768\n",
            "\t MAXIMUM_SURFACE1D_WIDTH: 32768\n",
            "\t MAXIMUM_SURFACE2D_HEIGHT: 65536\n",
            "\t MAXIMUM_SURFACE2D_LAYERED_HEIGHT: 32768\n",
            "\t MAXIMUM_SURFACE2D_LAYERED_LAYERS: 2048\n",
            "\t MAXIMUM_SURFACE2D_LAYERED_WIDTH: 32768\n",
            "\t MAXIMUM_SURFACE2D_WIDTH: 131072\n",
            "\t MAXIMUM_SURFACE3D_DEPTH: 16384\n",
            "\t MAXIMUM_SURFACE3D_HEIGHT: 16384\n",
            "\t MAXIMUM_SURFACE3D_WIDTH: 16384\n",
            "\t MAXIMUM_SURFACECUBEMAP_LAYERED_LAYERS: 2046\n",
            "\t MAXIMUM_SURFACECUBEMAP_LAYERED_WIDTH: 32768\n",
            "\t MAXIMUM_SURFACECUBEMAP_WIDTH: 32768\n",
            "\t MAXIMUM_TEXTURE1D_LAYERED_LAYERS: 2048\n",
            "\t MAXIMUM_TEXTURE1D_LAYERED_WIDTH: 32768\n",
            "\t MAXIMUM_TEXTURE1D_LINEAR_WIDTH: 268435456\n",
            "\t MAXIMUM_TEXTURE1D_MIPMAPPED_WIDTH: 32768\n",
            "\t MAXIMUM_TEXTURE1D_WIDTH: 131072\n",
            "\t MAXIMUM_TEXTURE2D_ARRAY_HEIGHT: 32768\n",
            "\t MAXIMUM_TEXTURE2D_ARRAY_NUMSLICES: 2048\n",
            "\t MAXIMUM_TEXTURE2D_ARRAY_WIDTH: 32768\n",
            "\t MAXIMUM_TEXTURE2D_GATHER_HEIGHT: 32768\n",
            "\t MAXIMUM_TEXTURE2D_GATHER_WIDTH: 32768\n",
            "\t MAXIMUM_TEXTURE2D_HEIGHT: 65536\n",
            "\t MAXIMUM_TEXTURE2D_LINEAR_HEIGHT: 65000\n",
            "\t MAXIMUM_TEXTURE2D_LINEAR_PITCH: 2097120\n",
            "\t MAXIMUM_TEXTURE2D_LINEAR_WIDTH: 131072\n",
            "\t MAXIMUM_TEXTURE2D_MIPMAPPED_HEIGHT: 32768\n",
            "\t MAXIMUM_TEXTURE2D_MIPMAPPED_WIDTH: 32768\n",
            "\t MAXIMUM_TEXTURE2D_WIDTH: 131072\n",
            "\t MAXIMUM_TEXTURE3D_DEPTH: 16384\n",
            "\t MAXIMUM_TEXTURE3D_DEPTH_ALTERNATE: 32768\n",
            "\t MAXIMUM_TEXTURE3D_HEIGHT: 16384\n",
            "\t MAXIMUM_TEXTURE3D_HEIGHT_ALTERNATE: 8192\n",
            "\t MAXIMUM_TEXTURE3D_WIDTH: 16384\n",
            "\t MAXIMUM_TEXTURE3D_WIDTH_ALTERNATE: 8192\n",
            "\t MAXIMUM_TEXTURECUBEMAP_LAYERED_LAYERS: 2046\n",
            "\t MAXIMUM_TEXTURECUBEMAP_LAYERED_WIDTH: 32768\n",
            "\t MAXIMUM_TEXTURECUBEMAP_WIDTH: 32768\n",
            "\t MAX_BLOCKS_PER_MULTIPROCESSOR: 16\n",
            "\t MAX_BLOCK_DIM_X: 1024\n",
            "\t MAX_BLOCK_DIM_Y: 1024\n",
            "\t MAX_BLOCK_DIM_Z: 64\n",
            "\t MAX_GRID_DIM_X: 2147483647\n",
            "\t MAX_GRID_DIM_Y: 65535\n",
            "\t MAX_GRID_DIM_Z: 65535\n",
            "\t MAX_PERSISTING_L2_CACHE_SIZE: 0\n",
            "\t MAX_PITCH: 2147483647\n",
            "\t MAX_REGISTERS_PER_BLOCK: 65536\n",
            "\t MAX_REGISTERS_PER_MULTIPROCESSOR: 65536\n",
            "\t MAX_SHARED_MEMORY_PER_BLOCK: 49152\n",
            "\t MAX_SHARED_MEMORY_PER_BLOCK_OPTIN: 65536\n",
            "\t MAX_SHARED_MEMORY_PER_MULTIPROCESSOR: 65536\n",
            "\t MAX_THREADS_PER_BLOCK: 1024\n",
            "\t MAX_THREADS_PER_MULTIPROCESSOR: 1024\n",
            "\t MEMORY_CLOCK_RATE: 5001000\n",
            "\t MEMORY_POOLS_SUPPORTED: 1\n",
            "\t MULTI_GPU_BOARD: 0\n",
            "\t MULTI_GPU_BOARD_GROUP_ID: 0\n",
            "\t PAGEABLE_MEMORY_ACCESS: 0\n",
            "\t PAGEABLE_MEMORY_ACCESS_USES_HOST_PAGE_TABLES: 0\n",
            "\t PCI_BUS_ID: 0\n",
            "\t PCI_DEVICE_ID: 4\n",
            "\t PCI_DOMAIN_ID: 0\n",
            "\t READ_ONLY_HOST_REGISTER_SUPPORTED: 1\n",
            "\t RESERVED_SHARED_MEMORY_PER_BLOCK: 0\n",
            "\t SINGLE_TO_DOUBLE_PRECISION_PERF_RATIO: 32\n",
            "\t STREAM_PRIORITIES_SUPPORTED: 1\n",
            "\t SURFACE_ALIGNMENT: 512\n",
            "\t TCC_DRIVER: 0\n",
            "\t TEXTURE_ALIGNMENT: 512\n",
            "\t TEXTURE_PITCH_ALIGNMENT: 32\n",
            "\t TOTAL_CONSTANT_MEMORY: 65536\n",
            "\t UNIFIED_ADDRESSING: 1\n",
            "\t WARP_SIZE: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-g92eSBn5FlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6941f367-84f5-4c30-b3df-07144a409401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "import numpy\n",
        "\n",
        "a = numpy.array([[1,1,1,1],[1,1,1,1],[1,1,1,1],[1,1,1,1]], dtype=numpy.float32)\n",
        "\n",
        "data=numpy.zeros((64, 64), dtype='int')\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u3G_VnAqYGVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92L9E7WkJhsy"
      },
      "outputs": [],
      "source": [
        "a_gpu = cuda.mem_alloc(a.nbytes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1xaAt_NJjSb"
      },
      "outputs": [],
      "source": [
        "cuda.memcpy_htod(a_gpu, a)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##pycuda index test"
      ],
      "metadata": {
        "id": "nMvWV6GeJY1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile a.py\n",
        "#!python \n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "from pycuda import gpuarray\n",
        "import numpy as np\n",
        "\n",
        "mod = SourceModule(\"\"\"\n",
        "    #include <stdio.h>\n",
        "\n",
        "    __global__ void say_hi(int *out_gpu)\n",
        "    { \n",
        "      int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "      int idy = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "      __shared__ double out_buf[1024];\n",
        "      int l_count = 0;\n",
        "      __shared__ int g_count;\n",
        "      l_count +=1;\n",
        "\n",
        "      printf(\"I am threadIdx:[%d[%d]  of block:[%d][%d] block size:[%d][%d][%d]  grid size:[%d][%d]:\\\\n\", threadIdx.x, threadIdx.y, blockIdx.x,blockIdx.y,blockDim.x,blockDim.y,blockDim.z, gridDim.x,gridDim.y);\n",
        "      \n",
        "\n",
        "      out_buf[idx]=idy;\n",
        "\n",
        "      __syncthreads();\n",
        "      g_count +=1;\n",
        "      //how to pass out the modified array data\n",
        "      __syncthreads();\n",
        "      out_gpu[idx]=2;\n",
        "      __syncthreads();\n",
        "      printf(\"thread id: [%d][%d]   -> l_count=%d ,g_count=%d *out_gpu=%d\\\\n\",idx,idy,l_count,g_count,*out_gpu);\n",
        "    }\n",
        "    \"\"\")\n",
        "\n",
        "func = mod.get_function(\"say_hi\")\n",
        "\n",
        "\n",
        "data=np.zeros(64, dtype='int')\n",
        "print('shape:',data.shape)\n",
        "data_gpu = gpuarray.to_gpu(data)\n",
        "out_gpu = gpuarray.empty_like(data_gpu)\n",
        "func(out_gpu,block=(3,1,1),grid=(1,1,1))\n",
        "print('modify pointer array:',out_gpu.get())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78Ed3tu7G2qf",
        "outputId": "bf982511-f9a0-43da-a8cc-ee29eec56a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting a.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python a.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUCbHjV7IwkW",
        "outputId": "23854314-625b-404a-cad7-d363f65e06d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (64,)\n",
            "I am threadIdx:[0[0]  of block:[0][0] block size:[3][1][1]  grid size:[1][1]:\n",
            "I am threadIdx:[1[0]  of block:[0][0] block size:[3][1][1]  grid size:[1][1]:\n",
            "I am threadIdx:[2[0]  of block:[0][0] block size:[3][1][1]  grid size:[1][1]:\n",
            "thread id: [0][0]   -> l_count=1 ,g_count=0 *out_gpu=2\n",
            "thread id: [1][0]   -> l_count=1 ,g_count=0 *out_gpu=2\n",
            "thread id: [2][0]   -> l_count=1 ,g_count=0 *out_gpu=2\n",
            "modify pointer array: [8589934594          2          0          0          0          0\n",
            "          0          0          0          0          0          0\n",
            "          0          0          0          0          0          0\n",
            "          0          0          0          0          0          0\n",
            "          0          0          0          0          0          0\n",
            "          0          0          0          0          0          0\n",
            "          0          0          0          0          0          0\n",
            "          0          0          0          0          0          0\n",
            "          0          0          0          0          0          0\n",
            "          0          0          0          0          0          0\n",
            "          0          0          0          0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile sh.py\n",
        "#!python \n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "from pycuda import gpuarray\n",
        "import numpy as np\n",
        "\n",
        "mod = SourceModule(\"\"\"\n",
        "    #include <stdio.h>\n",
        "\n",
        "    __global__ void say_hi(int *out_gpu)\n",
        "    { \n",
        "      int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "      int idy = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "\n",
        "      printf(\"I am threadIdx:[%d][%d]  of block:[%d][%d] block size:[%d][%d][%d]  grid size:[%d][%d]  idx=%d, idy=%d\\\\n\", threadIdx.x, threadIdx.y, blockIdx.x,blockIdx.y,blockDim.x,blockDim.y,blockDim.z, gridDim.x,gridDim.y,idx,idy);\n",
        "      //for(int i<0;i<3;i++){\n",
        "          printf(\"%d\\\\n\",out_gpu[idx*3+idy]);\n",
        "          //printf(\"%d\\\\n\",out_gpu[idx]);\n",
        "      //}\n",
        "     // __syncthreads();\n",
        "  \n",
        "    }\n",
        "    \"\"\")\n",
        "\n",
        "func = mod.get_function(\"say_hi\")\n",
        "\n",
        "\n",
        "data=np.array([21,2,3,38,9,10], dtype=np.int32)\n",
        "data=data.reshape(2,3)\n",
        "print('shape:',data.shape,data)\n",
        "data_gpu = gpuarray.to_gpu(data)\n",
        "func(data_gpu,block=(2,3,1),grid=(1,1,1))\n",
        "#print('modify pointer array:',data_gpu.get())"
      ],
      "metadata": {
        "id": "hK9KLy3OYOB6",
        "outputId": "1beb79a2-eec3-48c3-e2f2-2a6cb0156e9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting sh.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python sh.py"
      ],
      "metadata": {
        "id": "x5RLdC16YxRA",
        "outputId": "f69490cc-7bbf-4600-b7ba-2b3b65bc9c1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (2, 3) [[21  2  3]\n",
            " [38  9 10]]\n",
            "I am threadIdx:[0][0]  of block:[0][0] block size:[2][3][1]  grid size:[1][1]  idx=0, idy=0\n",
            "I am threadIdx:[1][0]  of block:[0][0] block size:[2][3][1]  grid size:[1][1]  idx=1, idy=0\n",
            "I am threadIdx:[0][1]  of block:[0][0] block size:[2][3][1]  grid size:[1][1]  idx=0, idy=1\n",
            "I am threadIdx:[1][1]  of block:[0][0] block size:[2][3][1]  grid size:[1][1]  idx=1, idy=1\n",
            "I am threadIdx:[0][2]  of block:[0][0] block size:[2][3][1]  grid size:[1][1]  idx=0, idy=2\n",
            "I am threadIdx:[1][2]  of block:[0][0] block size:[2][3][1]  grid size:[1][1]  idx=1, idy=2\n",
            "21\n",
            "38\n",
            "2\n",
            "9\n",
            "3\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "data=np.array([21,2,3,38,9,10], dtype=np.int32)\n",
        "print(data.reshape(2,3))\n",
        "print(data)"
      ],
      "metadata": {
        "id": "jn4CbsMNxEHT",
        "outputId": "1a595ee6-3ce1-48f8-ecbb-28e3c314537c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[21  2  3]\n",
            " [38  9 10]]\n",
            "[21  2  3 38  9 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run naive_prefix.py"
      ],
      "metadata": {
        "id": "VVzrqW9wKBZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pycuda.autoinit\n",
        "import pycuda.driver as drv\n",
        "import numpy as np\n",
        "from pycuda import gpuarray\n",
        "from pycuda.compiler import SourceModule\n",
        "from time import time\n",
        "# this is a naive parallel prefix-sum kernel that uses shared memory\n",
        "naive_ker = SourceModule(\"\"\"\n",
        "__global__ void naive_prefix(double *vec, double *out)\n",
        "{\n",
        "     __shared__ double sum_buf[1024];     \n",
        "     int tid = threadIdx.x;     \n",
        "     sum_buf[tid] = vec[tid];\n",
        "     \n",
        "     // begin parallel prefix sum algorithm\n",
        "     \n",
        "     int iter = 1;\n",
        "     for (int i=0; i < 10; i++)\n",
        "     {\n",
        "         __syncthreads();\n",
        "         if (tid >= iter )\n",
        "         {\n",
        "             sum_buf[tid] = sum_buf[tid] + sum_buf[tid - iter];            \n",
        "         }\n",
        "         \n",
        "         iter *= 2;\n",
        "     }\n",
        "         \n",
        "    __syncthreads();\n",
        "    out[tid] = sum_buf[tid];\n",
        "    __syncthreads();\n",
        "        \n",
        "}\n",
        "\"\"\")\n",
        "naive_gpu = naive_ker.get_function(\"naive_prefix\")\n",
        "    \n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    \n",
        "    testvec = np.random.randn(1024).astype(np.float64)\n",
        "    testvec_gpu = gpuarray.to_gpu(testvec)\n",
        "    \n",
        "    outvec_gpu = gpuarray.empty_like(testvec_gpu)\n",
        "\n",
        "    naive_gpu( testvec_gpu , outvec_gpu, block=(1024,1,1), grid=(1,1,1))\n",
        "    \n",
        "    total_sum = sum( testvec)\n",
        "    total_sum_gpu = outvec_gpu[-1].get()\n",
        "    \n",
        "    print('outvec_gpu:',outvec_gpu)\n",
        "    print(\"Does our kernel work correctly? : {}\".format(np.allclose(total_sum_gpu , total_sum) ))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2B8sQV2H6Ns",
        "outputId": "51e30bbd-fc65-4d78-d5ee-bcd074bf7bb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "outvec_gpu: [ -1.20500879  -1.6132585   -2.71519557 ... -57.63849801 -58.57298875\n",
            " -59.34142221]\n",
            "Does our kernel work correctly? : True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### hello world\n",
        "https://documen.tician.de/pycuda/tutorial.html"
      ],
      "metadata": {
        "id": "iJFY8s_TbvAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPrAk7z1b4Fv",
        "outputId": "361f794d-9856-4538-e974-d782ea423c6c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0 0]\n",
            " [1 1 0]\n",
            " [0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### print input"
      ],
      "metadata": {
        "id": "cinZJ7SOOoaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile i.py\n",
        "#!python \n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "from pycuda import gpuarray\n",
        "import numpy as np\n",
        "\n",
        "mod = SourceModule(\"\"\"\n",
        "    #include <stdio.h>\n",
        "\n",
        "    __global__ void say_hi(int *out_gpu)\n",
        "    { \n",
        "      int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "     \n",
        "\n",
        "      printf(\"index: threadIdx.x =%d blockIdx.x =%d blockDim.x %d  \\\\n\", threadIdx.x , blockIdx.x , blockDim.x);\n",
        "      printf(\"thread id: [%d]  intput out_gpu=%d \\\\n\",idx,out_gpu[idx]);\n",
        "      \n",
        "      out_gpu[idx] *= 2;\n",
        "      printf(\"thread id 2: [%d]  out_gpu=%d \\\\n\",idx,out_gpu[idx]);\n",
        "\n",
        "    }\n",
        "    \"\"\")\n",
        "\n",
        "func = mod.get_function(\"say_hi\")\n",
        "\n",
        "\n",
        "t= np.array([1,2,3,4], dtype=np.int32)\n",
        "print('shape:',t.shape,t.size)\n",
        "t_gpu = gpuarray.to_gpu(t)\n",
        "func(t_gpu,block=(t.size,1,1),grid=(1,1,1))\n",
        "print(t_gpu.get())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhN00rk1OlsB",
        "outputId": "41439a09-ea34-4d39-b9b5-b6b48e7ba7d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting i.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python i.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34rSfTS-PILE",
        "outputId": "f33576f1-00cd-48ec-cdf3-f2b28529e5d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (4,) 4\n",
            "index: threadIdx.x =0 blockIdx.x =0 blockDim.x 4  \n",
            "index: threadIdx.x =1 blockIdx.x =0 blockDim.x 4  \n",
            "index: threadIdx.x =2 blockIdx.x =0 blockDim.x 4  \n",
            "index: threadIdx.x =3 blockIdx.x =0 blockDim.x 4  \n",
            "thread id: [0]  intput out_gpu=1 \n",
            "thread id: [1]  intput out_gpu=2 \n",
            "thread id: [2]  intput out_gpu=3 \n",
            "thread id: [3]  intput out_gpu=4 \n",
            "thread id 2: [0]  out_gpu=2 \n",
            "thread id 2: [1]  out_gpu=4 \n",
            "thread id 2: [2]  out_gpu=6 \n",
            "thread id 2: [3]  out_gpu=8 \n",
            "[2 4 6 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pass string to kernel"
      ],
      "metadata": {
        "id": "J7vgFyC2LOG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "s= np.array(['abc','hello,world'], dtype=object)\n",
        "print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4kfdzClLkuI",
        "outputId": "9cdc590d-5302-488e-b77a-39fa1a16e9b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['abc' 'hello,world']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile s.py\n",
        "#!python \n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "from pycuda import gpuarray\n",
        "import numpy as np\n",
        "\n",
        "mod = SourceModule(\"\"\"\n",
        "    #include <stdio.h>\n",
        "\n",
        "    __global__ void say_hi(int *out_gpu)\n",
        "    { \n",
        "      int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "      printf(\"thread id: [%d]  %d\\\\n\",idx,out_gpu);\n",
        "      //print c-str\n",
        "      for (int i=0;i<5;i++){\n",
        "        printf(\"%c\",out_gpu[0]);\n",
        "      }\n",
        "      printf(\" >end\\\\n\");\n",
        "\n",
        "    }\n",
        "    \"\"\")\n",
        "\n",
        "func = mod.get_function(\"say_hi\")\n",
        "\n",
        "\n",
        "s= np.array(['abcdf'], dtype=object)\n",
        "print('shape:',s.shape)\n",
        "print(s,s.data)\n",
        "s_gpu = gpuarray.to_gpu(s)\n",
        "func(s_gpu,block=(s.size,1,1),grid=(1,1,1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQy3qUsQLSDv",
        "outputId": "d95b3e4f-1eae-44df-e438-e3fce109bde2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting s.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python s.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3fsxgNvM8Rq",
        "outputId": "e7ae4cc4-16ec-48e3-f643-af5c3c6867dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/s.py:8: UserWarning: The CUDA compiler succeeded, but said the following:\n",
            "kernel.cu(8): warning #181-D: argument is incompatible with corresponding format string conversion\n",
            "\n",
            "\n",
            "  mod = SourceModule(\"\"\"\n",
            "shape: (1,)\n",
            "['abcdf'] <memory at 0x7f66026f9640>\n",
            "thread id: [0]  0\n",
            "����� >end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQYD348qKgva"
      },
      "outputs": [],
      "source": [
        "mod2 = SourceModule(\"\"\"\n",
        "  __global__ void add2(float *a, float *b)\n",
        "  {\n",
        "    int idx = threadIdx.x + threadIdx.y*4;\n",
        "    a[idx] += b[idx];\n",
        "  }\n",
        "  \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vs_5Hb-6Kr-C"
      },
      "outputs": [],
      "source": [
        "b_gpu = cuda.mem_alloc(b.nbytes)\n",
        "c_gpu = cuda.mem_alloc(c.nbytes)\n",
        "\n",
        "cuda.memcpy_htod(b_gpu, b)\n",
        "cuda.memcpy_htod(c_gpu, c)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwA4tpOtLE5_"
      },
      "outputs": [],
      "source": [
        "func = mod2.get_function(\"add2\")\n",
        "func(b_gpu,c_gpu, block=(4,4,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zN8iBYDM_00"
      },
      "outputs": [],
      "source": [
        "added = numpy.empty_like(b)\n",
        "cuda.memcpy_dtoh(added, b_gpu)\n",
        "print(added)\n",
        "print(b)\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJBVoR8ANgx5"
      },
      "source": [
        "# Exercises\n",
        "\n",
        "1. Write a cuda kernel to find the elementwise square of a matrix\n",
        "2. Write a cuda kernel to find a matrix, which when added to the given matrix results in every element being equal to zero\n",
        "3. Write a cuda kernel to multiply two matrices:\n",
        "    1. Assume square matrices, with dimensions < 1024\n",
        "    2. Assume square matrices, with dimensions > 1024\n",
        "    3. Assume non-square matrices, with dimensions > 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBYr6BUWNuLe"
      },
      "outputs": [],
      "source": [
        "1. "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}