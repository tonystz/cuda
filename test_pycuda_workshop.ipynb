{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonystz/gitpod/blob/main/test_pycuda_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2JZt1GL5D6W"
      },
      "source": [
        "# Introduction to CUDA and PyCUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FA_YN7HlGRP5"
      },
      "outputs": [],
      "source": [
        "!pip install pycuda # install cuda\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## query info"
      ],
      "metadata": {
        "id": "buBAOxuMIqhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "O45Aq6Bzzcc-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ced442c7-b122-4aaa-da6f-094d2d32d6a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Mar 14 11:40:25 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P0    28W /  70W |    103MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pycuda\n",
        "import pycuda.driver as drv\n",
        "drv.init()\n",
        "\n",
        "print('CUDA device query (PyCUDA version) \\n')\n",
        "\n",
        "print('Detected {} CUDA Capable device(s) \\n'.format(drv.Device.count()))\n",
        "\n",
        "for i in range(drv.Device.count()):\n",
        "    \n",
        "    gpu_device = drv.Device(i)\n",
        "    print('Device {}: {}'.format( i, gpu_device.name() )) \n",
        "    compute_capability = float( '%d.%d' % gpu_device.compute_capability() )\n",
        "    print('\\t Compute Capability: {}'.format(compute_capability))\n",
        "    print('\\t Total Memory: {} megabytes'.format(gpu_device.total_memory()//(1024**2)))\n",
        "\n",
        "\n",
        "    # The following will give us all remaining device attributes as seen \n",
        "    # in the original deviceQuery.\n",
        "    # We set up a dictionary as such so that we can easily index\n",
        "    # the values using a string descriptor.\n",
        "    \n",
        "    device_attributes_tuples = iter(gpu_device.get_attributes().items()) \n",
        "    device_attributes = {}\n",
        "\n",
        "        \n",
        "    for k, v in device_attributes_tuples:\n",
        "        device_attributes[str(k)] = v\n",
        "        # print(f'{k}->{v}')\n",
        "    # continue\n",
        "    num_mp = device_attributes['MULTIPROCESSOR_COUNT']\n",
        "    \n",
        "    # Cores per multiprocessor is not reported by the GPU!  \n",
        "    # We must use a lookup table based on compute capability.\n",
        "    # See the following:\n",
        "    # http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities\n",
        "    \n",
        "    cuda_cores_per_mp = { 5.0 : 128, 5.1 : 128, 5.2 : 128, 6.0 : 64, 6.1 : 128, 7.5 : 128}[compute_capability]\n",
        "    \n",
        "    print('\\t ({}) Multiprocessors, ({}) CUDA Cores / Multiprocessor: {} CUDA Cores'.format(num_mp, cuda_cores_per_mp, num_mp*cuda_cores_per_mp))\n",
        "    \n",
        "    device_attributes.pop('MULTIPROCESSOR_COUNT')\n",
        "    \n",
        "    for k in list(device_attributes.keys()):\n",
        "        print('\\t {}: {}'.format(k, device_attributes[k]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkYg1h5iF-x6",
        "outputId": "c4d0876a-55f5-436f-94ca-b5fb179988be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA device query (PyCUDA version) \n",
            "\n",
            "Detected 1 CUDA Capable device(s) \n",
            "\n",
            "Device 0: Tesla T4\n",
            "\t Compute Capability: 7.5\n",
            "\t Total Memory: 15101 megabytes\n",
            "\t (40) Multiprocessors, (128) CUDA Cores / Multiprocessor: 5120 CUDA Cores\n",
            "\t ASYNC_ENGINE_COUNT: 3\n",
            "\t CAN_MAP_HOST_MEMORY: 1\n",
            "\t CAN_USE_HOST_POINTER_FOR_REGISTERED_MEM: 1\n",
            "\t CLOCK_RATE: 1590000\n",
            "\t COMPUTE_CAPABILITY_MAJOR: 7\n",
            "\t COMPUTE_CAPABILITY_MINOR: 5\n",
            "\t COMPUTE_MODE: DEFAULT\n",
            "\t COMPUTE_PREEMPTION_SUPPORTED: 1\n",
            "\t CONCURRENT_KERNELS: 1\n",
            "\t CONCURRENT_MANAGED_ACCESS: 1\n",
            "\t DIRECT_MANAGED_MEM_ACCESS_FROM_HOST: 0\n",
            "\t ECC_ENABLED: 1\n",
            "\t GENERIC_COMPRESSION_SUPPORTED: 0\n",
            "\t GLOBAL_L1_CACHE_SUPPORTED: 1\n",
            "\t GLOBAL_MEMORY_BUS_WIDTH: 256\n",
            "\t GPU_OVERLAP: 1\n",
            "\t HANDLE_TYPE_POSIX_FILE_DESCRIPTOR_SUPPORTED: 1\n",
            "\t HANDLE_TYPE_WIN32_HANDLE_SUPPORTED: 0\n",
            "\t HANDLE_TYPE_WIN32_KMT_HANDLE_SUPPORTED: 0\n",
            "\t HOST_NATIVE_ATOMIC_SUPPORTED: 0\n",
            "\t INTEGRATED: 0\n",
            "\t KERNEL_EXEC_TIMEOUT: 0\n",
            "\t L2_CACHE_SIZE: 4194304\n",
            "\t LOCAL_L1_CACHE_SUPPORTED: 1\n",
            "\t MANAGED_MEMORY: 1\n",
            "\t MAXIMUM_SURFACE1D_LAYERED_LAYERS: 2048\n",
            "\t MAXIMUM_SURFACE1D_LAYERED_WIDTH: 32768\n",
            "\t MAXIMUM_SURFACE1D_WIDTH: 32768\n",
            "\t MAXIMUM_SURFACE2D_HEIGHT: 65536\n",
            "\t MAXIMUM_SURFACE2D_LAYERED_HEIGHT: 32768\n",
            "\t MAXIMUM_SURFACE2D_LAYERED_LAYERS: 2048\n",
            "\t MAXIMUM_SURFACE2D_LAYERED_WIDTH: 32768\n",
            "\t MAXIMUM_SURFACE2D_WIDTH: 131072\n",
            "\t MAXIMUM_SURFACE3D_DEPTH: 16384\n",
            "\t MAXIMUM_SURFACE3D_HEIGHT: 16384\n",
            "\t MAXIMUM_SURFACE3D_WIDTH: 16384\n",
            "\t MAXIMUM_SURFACECUBEMAP_LAYERED_LAYERS: 2046\n",
            "\t MAXIMUM_SURFACECUBEMAP_LAYERED_WIDTH: 32768\n",
            "\t MAXIMUM_SURFACECUBEMAP_WIDTH: 32768\n",
            "\t MAXIMUM_TEXTURE1D_LAYERED_LAYERS: 2048\n",
            "\t MAXIMUM_TEXTURE1D_LAYERED_WIDTH: 32768\n",
            "\t MAXIMUM_TEXTURE1D_LINEAR_WIDTH: 268435456\n",
            "\t MAXIMUM_TEXTURE1D_MIPMAPPED_WIDTH: 32768\n",
            "\t MAXIMUM_TEXTURE1D_WIDTH: 131072\n",
            "\t MAXIMUM_TEXTURE2D_ARRAY_HEIGHT: 32768\n",
            "\t MAXIMUM_TEXTURE2D_ARRAY_NUMSLICES: 2048\n",
            "\t MAXIMUM_TEXTURE2D_ARRAY_WIDTH: 32768\n",
            "\t MAXIMUM_TEXTURE2D_GATHER_HEIGHT: 32768\n",
            "\t MAXIMUM_TEXTURE2D_GATHER_WIDTH: 32768\n",
            "\t MAXIMUM_TEXTURE2D_HEIGHT: 65536\n",
            "\t MAXIMUM_TEXTURE2D_LINEAR_HEIGHT: 65000\n",
            "\t MAXIMUM_TEXTURE2D_LINEAR_PITCH: 2097120\n",
            "\t MAXIMUM_TEXTURE2D_LINEAR_WIDTH: 131072\n",
            "\t MAXIMUM_TEXTURE2D_MIPMAPPED_HEIGHT: 32768\n",
            "\t MAXIMUM_TEXTURE2D_MIPMAPPED_WIDTH: 32768\n",
            "\t MAXIMUM_TEXTURE2D_WIDTH: 131072\n",
            "\t MAXIMUM_TEXTURE3D_DEPTH: 16384\n",
            "\t MAXIMUM_TEXTURE3D_DEPTH_ALTERNATE: 32768\n",
            "\t MAXIMUM_TEXTURE3D_HEIGHT: 16384\n",
            "\t MAXIMUM_TEXTURE3D_HEIGHT_ALTERNATE: 8192\n",
            "\t MAXIMUM_TEXTURE3D_WIDTH: 16384\n",
            "\t MAXIMUM_TEXTURE3D_WIDTH_ALTERNATE: 8192\n",
            "\t MAXIMUM_TEXTURECUBEMAP_LAYERED_LAYERS: 2046\n",
            "\t MAXIMUM_TEXTURECUBEMAP_LAYERED_WIDTH: 32768\n",
            "\t MAXIMUM_TEXTURECUBEMAP_WIDTH: 32768\n",
            "\t MAX_BLOCKS_PER_MULTIPROCESSOR: 16\n",
            "\t MAX_BLOCK_DIM_X: 1024\n",
            "\t MAX_BLOCK_DIM_Y: 1024\n",
            "\t MAX_BLOCK_DIM_Z: 64\n",
            "\t MAX_GRID_DIM_X: 2147483647\n",
            "\t MAX_GRID_DIM_Y: 65535\n",
            "\t MAX_GRID_DIM_Z: 65535\n",
            "\t MAX_PERSISTING_L2_CACHE_SIZE: 0\n",
            "\t MAX_PITCH: 2147483647\n",
            "\t MAX_REGISTERS_PER_BLOCK: 65536\n",
            "\t MAX_REGISTERS_PER_MULTIPROCESSOR: 65536\n",
            "\t MAX_SHARED_MEMORY_PER_BLOCK: 49152\n",
            "\t MAX_SHARED_MEMORY_PER_BLOCK_OPTIN: 65536\n",
            "\t MAX_SHARED_MEMORY_PER_MULTIPROCESSOR: 65536\n",
            "\t MAX_THREADS_PER_BLOCK: 1024\n",
            "\t MAX_THREADS_PER_MULTIPROCESSOR: 1024\n",
            "\t MEMORY_CLOCK_RATE: 5001000\n",
            "\t MEMORY_POOLS_SUPPORTED: 1\n",
            "\t MULTI_GPU_BOARD: 0\n",
            "\t MULTI_GPU_BOARD_GROUP_ID: 0\n",
            "\t PAGEABLE_MEMORY_ACCESS: 0\n",
            "\t PAGEABLE_MEMORY_ACCESS_USES_HOST_PAGE_TABLES: 0\n",
            "\t PCI_BUS_ID: 0\n",
            "\t PCI_DEVICE_ID: 4\n",
            "\t PCI_DOMAIN_ID: 0\n",
            "\t READ_ONLY_HOST_REGISTER_SUPPORTED: 1\n",
            "\t RESERVED_SHARED_MEMORY_PER_BLOCK: 0\n",
            "\t SINGLE_TO_DOUBLE_PRECISION_PERF_RATIO: 32\n",
            "\t STREAM_PRIORITIES_SUPPORTED: 1\n",
            "\t SURFACE_ALIGNMENT: 512\n",
            "\t TCC_DRIVER: 0\n",
            "\t TEXTURE_ALIGNMENT: 512\n",
            "\t TEXTURE_PITCH_ALIGNMENT: 32\n",
            "\t TOTAL_CONSTANT_MEMORY: 65536\n",
            "\t UNIFIED_ADDRESSING: 1\n",
            "\t WARP_SIZE: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##pycuda index test"
      ],
      "metadata": {
        "id": "nMvWV6GeJY1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile a.py\n",
        "#!python \n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "from pycuda import gpuarray\n",
        "import numpy as np\n",
        "\n",
        "mod = SourceModule(\"\"\"\n",
        "    #include <stdio.h>\n",
        "\n",
        "    __global__ void say_hi(int *out_gpu)\n",
        "    { \n",
        "      int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "      int idy = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "      __shared__ double out_buf[1024];\n",
        "      int l_count = 0;\n",
        "      __shared__ int g_count;\n",
        "      l_count +=1;\n",
        "\n",
        "      printf(\"I am threadIdx:[%d[%d]  of block:[%d][%d] block size:[%d][%d][%d]  grid size:[%d][%d]:\\\\n\", threadIdx.x, threadIdx.y, blockIdx.x,blockIdx.y,blockDim.x,blockDim.y,blockDim.z, gridDim.x,gridDim.y);\n",
        "      \n",
        "\n",
        "      out_buf[idx]=idy;\n",
        "\n",
        "      __syncthreads();\n",
        "      g_count +=1;\n",
        "      //how to pass out the modified array data\n",
        "      __syncthreads();\n",
        "      out_gpu[idx]=2;\n",
        "      __syncthreads();\n",
        "      printf(\"thread id: [%d][%d]   -> l_count=%d ,g_count=%d *out_gpu=%d\\\\n\",idx,idy,l_count,g_count,*out_gpu);\n",
        "    }\n",
        "    \"\"\")\n",
        "\n",
        "func = mod.get_function(\"say_hi\")\n",
        "\n",
        "\n",
        "data=np.zeros(64, dtype=np.int32)\n",
        "print('shape:',data.shape)\n",
        "data_gpu = gpuarray.to_gpu(data)\n",
        "out_gpu = gpuarray.empty_like(data_gpu)\n",
        "func(out_gpu,block=(3,1,1),grid=(1,1,1))\n",
        "print('modify pointer array:',out_gpu.get())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78Ed3tu7G2qf",
        "outputId": "b41762e0-1e1b-46f5-b5c4-72955f7f79c6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing a.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python a.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUCbHjV7IwkW",
        "outputId": "ad36d6e5-cd1f-4a9e-eb51-ed101b179bae"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/a.py:8: UserWarning: The CUDA compiler succeeded, but said the following:\n",
            "kernel.cu(9): warning #550-D: variable \"out_buf\" was set but never used\n",
            "\n",
            "\n",
            "  mod = SourceModule(\"\"\"\n",
            "shape: (64,)\n",
            "I am threadIdx:[0[0]  of block:[0][0] block size:[3][1][1]  grid size:[1][1]:\n",
            "I am threadIdx:[1[0]  of block:[0][0] block size:[3][1][1]  grid size:[1][1]:\n",
            "I am threadIdx:[2[0]  of block:[0][0] block size:[3][1][1]  grid size:[1][1]:\n",
            "thread id: [0][0]   -> l_count=1 ,g_count=0 *out_gpu=2\n",
            "thread id: [1][0]   -> l_count=1 ,g_count=0 *out_gpu=2\n",
            "thread id: [2][0]   -> l_count=1 ,g_count=0 *out_gpu=2\n",
            "modify pointer array: [2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##test 1d array index:one row, colSize thread"
      ],
      "metadata": {
        "id": "wuAdTUGqOl3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile a1colThead.py\n",
        "#!python \n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "from pycuda import gpuarray\n",
        "import numpy as np\n",
        "\n",
        "mod = SourceModule(\"\"\"\n",
        "    #include <stdio.h>\n",
        "\n",
        "    __global__ void say_hi(int *out_gpu)\n",
        "    { \n",
        "      int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "      int idy = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "      const int colSize=6;\n",
        "\n",
        "      printf(\"I am threadIdx:[%d][%d]  of block:[%d][%d] block size:[%d][%d][%d]  grid size:[%d][%d]  idx=%d, idy=%d\\\\n\", \\\n",
        "      threadIdx.x, threadIdx.y, blockIdx.x,blockIdx.y,blockDim.x,blockDim.y,blockDim.z, gridDim.x,gridDim.y,idx,idy);\n",
        "      printf(\"perThadloop[%d]: %d\\\\n\",idx,out_gpu[idx]);\n",
        "      for(int i=0;i<colSize;i++){\n",
        "          printf(\"one row/col thread:idx=%d, %d\\\\n\",idx,out_gpu[idx*colSize+i]);\n",
        "          //printf(\"%d\\\\n\",out_gpu[idx]);\n",
        "      }\n",
        "      //__syncthreads();\n",
        "  \n",
        "    }\n",
        "    \"\"\")\n",
        "\n",
        "func = mod.get_function(\"say_hi\")\n",
        "\n",
        "\n",
        "data=np.array([21,2,3,38,9,10], dtype=np.int32)\n",
        "#data=data.reshape(2,3)\n",
        "print('shape:',data.shape,data)\n",
        "data_gpu = gpuarray.to_gpu(data)\n",
        "func(data_gpu,block=(6,1,1),grid=(1,1,1))\n",
        "#print('modify pointer array:',data_gpu.get())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcy4FBiXOsSs",
        "outputId": "6f2178f2-22f5-44b9-d17d-9af7376e206a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting a1colThead.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python a1colThead.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wotsS1jsO2Vk",
        "outputId": "fd4bc04d-b14c-489d-bf8d-e4d970bd6d5d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (6,) [21  2  3 38  9 10]\n",
            "I am threadIdx:[0][0]  of block:[0][0] block size:[6][1][1]  grid size:[1][1]  idx=0, idy=0\n",
            "I am threadIdx:[1][0]  of block:[0][0] block size:[6][1][1]  grid size:[1][1]  idx=1, idy=0\n",
            "I am threadIdx:[2][0]  of block:[0][0] block size:[6][1][1]  grid size:[1][1]  idx=2, idy=0\n",
            "I am threadIdx:[3][0]  of block:[0][0] block size:[6][1][1]  grid size:[1][1]  idx=3, idy=0\n",
            "I am threadIdx:[4][0]  of block:[0][0] block size:[6][1][1]  grid size:[1][1]  idx=4, idy=0\n",
            "I am threadIdx:[5][0]  of block:[0][0] block size:[6][1][1]  grid size:[1][1]  idx=5, idy=0\n",
            "perThadloop[0]: 21\n",
            "perThadloop[1]: 2\n",
            "perThadloop[2]: 3\n",
            "perThadloop[3]: 38\n",
            "perThadloop[4]: 9\n",
            "perThadloop[5]: 10\n",
            "one row/col thread:idx=0, 21\n",
            "one row/col thread:idx=1, 0\n",
            "one row/col thread:idx=2, 0\n",
            "one row/col thread:idx=3, 0\n",
            "one row/col thread:idx=4, 0\n",
            "one row/col thread:idx=5, 0\n",
            "one row/col thread:idx=0, 2\n",
            "one row/col thread:idx=1, 0\n",
            "one row/col thread:idx=2, 0\n",
            "one row/col thread:idx=3, 0\n",
            "one row/col thread:idx=4, 0\n",
            "one row/col thread:idx=5, 0\n",
            "one row/col thread:idx=0, 3\n",
            "one row/col thread:idx=1, 0\n",
            "one row/col thread:idx=2, 0\n",
            "one row/col thread:idx=3, 0\n",
            "one row/col thread:idx=4, 0\n",
            "one row/col thread:idx=5, 0\n",
            "one row/col thread:idx=0, 38\n",
            "one row/col thread:idx=1, 0\n",
            "one row/col thread:idx=2, 0\n",
            "one row/col thread:idx=3, 0\n",
            "one row/col thread:idx=4, 0\n",
            "one row/col thread:idx=5, 0\n",
            "one row/col thread:idx=0, 9\n",
            "one row/col thread:idx=1, 0\n",
            "one row/col thread:idx=2, 0\n",
            "one row/col thread:idx=3, 0\n",
            "one row/col thread:idx=4, 0\n",
            "one row/col thread:idx=5, 0\n",
            "one row/col thread:idx=0, 10\n",
            "one row/col thread:idx=1, 0\n",
            "one row/col thread:idx=2, 0\n",
            "one row/col thread:idx=3, 0\n",
            "one row/col thread:idx=4, 0\n",
            "one row/col thread:idx=5, 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test 1d array index:one row, one thread"
      ],
      "metadata": {
        "id": "gkReiaL8NUea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile a1.py\n",
        "#!python \n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "from pycuda import gpuarray\n",
        "import numpy as np\n",
        "\n",
        "mod = SourceModule(\"\"\"\n",
        "    #include <stdio.h>\n",
        "\n",
        "    __global__ void say_hi(int *out_gpu)\n",
        "    { \n",
        "      int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "      int idy = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "      const int colSize=6;\n",
        "\n",
        "      printf(\"I am threadIdx:[%d][%d]  of block:[%d][%d] block size:[%d][%d][%d]  grid size:[%d][%d]  idx=%d, idy=%d\\\\n\", \\\n",
        "      threadIdx.x, threadIdx.y, blockIdx.x,blockIdx.y,blockDim.x,blockDim.y,blockDim.z, gridDim.x,gridDim.y,idx,idy);\n",
        "      for(int i=0;i<colSize;i++){\n",
        "          printf(\"one row/one thread:idx=%d, %d\\\\n\",idx,out_gpu[idx*colSize+i]);\n",
        "          //printf(\"%d\\\\n\",out_gpu[idx]);\n",
        "      }\n",
        "      //__syncthreads();\n",
        "  \n",
        "    }\n",
        "    \"\"\")\n",
        "\n",
        "func = mod.get_function(\"say_hi\")\n",
        "\n",
        "\n",
        "data=np.array([21,2,3,38,9,10], dtype=np.int32)\n",
        "#data=data.reshape(2,3)\n",
        "print('shape:',data.shape,data)\n",
        "data_gpu = gpuarray.to_gpu(data)\n",
        "func(data_gpu,block=(1,1,1),grid=(1,1,1))\n",
        "#print('modify pointer array:',data_gpu.get())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N164TGyXNYDy",
        "outputId": "a923b489-c6c9-452d-b857-2eb170449659"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting a1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python a1.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghLnI0GRNgMK",
        "outputId": "95328de6-0537-420d-d7ce-127582063428"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (6,) [21  2  3 38  9 10]\n",
            "I am threadIdx:[0][0]  of block:[0][0] block size:[1][1][1]  grid size:[1][1]  idx=0, idy=0\n",
            "one row/one thread:idx=0, 21\n",
            "one row/one thread:idx=0, 2\n",
            "one row/one thread:idx=0, 3\n",
            "one row/one thread:idx=0, 38\n",
            "one row/one thread:idx=0, 9\n",
            "one row/one thread:idx=0, 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test 2d arry index: one row, colSize thread"
      ],
      "metadata": {
        "id": "H260PHzNQ23L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile a2colSizeThread.py\n",
        "#!python \n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "from pycuda import gpuarray\n",
        "import numpy as np\n",
        "\n",
        "mod = SourceModule(\"\"\"\n",
        "    #include <stdio.h>\n",
        "\n",
        "    __global__ void say_hi(int *out_gpu)\n",
        "    { \n",
        "      int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "      int idy = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "\n",
        "      printf(\"I am threadIdx:[%d][%d]  of block:[%d][%d] block size:[%d][%d][%d]  grid size:[%d][%d]  idx=%d, idy=%d\\\\n\", \\\n",
        "      threadIdx.x, threadIdx.y, blockIdx.x,blockIdx.y,blockDim.x,blockDim.y,blockDim.z, gridDim.x,gridDim.y,idx,idy);\n",
        "      \n",
        "      printf(\"perThadloop[%d][%d]: %d\\\\n\",idx,idy,out_gpu[idx*3+idy]);\n",
        "\n",
        "      for(int i=0;i<3;i++){\n",
        "          printf(\"idx=%d, %d\\\\n\",idx,out_gpu[idx*3+i]);\n",
        "          //printf(\"%d\\\\n\",out_gpu[idx]);\n",
        "      }\n",
        "      //__syncthreads();\n",
        "  \n",
        "    }\n",
        "    \"\"\")\n",
        "\n",
        "func = mod.get_function(\"say_hi\")\n",
        "\n",
        "\n",
        "data=np.array([21,2,3,38,9,10], dtype=np.int32)\n",
        "data=data.reshape(2,3)\n",
        "print('shape:',data.shape,data)\n",
        "data_gpu = gpuarray.to_gpu(data)\n",
        "func(data_gpu,block=(2,3,1),grid=(1,1,1))\n",
        "#print('modify pointer array:',data_gpu.get())"
      ],
      "metadata": {
        "id": "zaOBTXWmQ_Og",
        "outputId": "1675f479-6dfa-401b-f441-d951bcb89db0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting a2colSizeThread.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python a2colSizeThread.py"
      ],
      "metadata": {
        "id": "r9YDgtM1RD0M",
        "outputId": "3107b9e0-92e8-4863-f1bd-27279345b65b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (2, 3) [[21  2  3]\n",
            " [38  9 10]]\n",
            "I am threadIdx:[0][0]  of block:[0][0] block size:[2][3][1]  grid size:[1][1]  idx=0, idy=0\n",
            "I am threadIdx:[1][0]  of block:[0][0] block size:[2][3][1]  grid size:[1][1]  idx=1, idy=0\n",
            "I am threadIdx:[0][1]  of block:[0][0] block size:[2][3][1]  grid size:[1][1]  idx=0, idy=1\n",
            "I am threadIdx:[1][1]  of block:[0][0] block size:[2][3][1]  grid size:[1][1]  idx=1, idy=1\n",
            "I am threadIdx:[0][2]  of block:[0][0] block size:[2][3][1]  grid size:[1][1]  idx=0, idy=2\n",
            "I am threadIdx:[1][2]  of block:[0][0] block size:[2][3][1]  grid size:[1][1]  idx=1, idy=2\n",
            "perThadloop[0][0]: 21\n",
            "perThadloop[1][0]: 38\n",
            "perThadloop[0][1]: 2\n",
            "perThadloop[1][1]: 9\n",
            "perThadloop[0][2]: 3\n",
            "perThadloop[1][2]: 10\n",
            "idx=0, 21\n",
            "idx=1, 38\n",
            "idx=0, 21\n",
            "idx=1, 38\n",
            "idx=0, 21\n",
            "idx=1, 38\n",
            "idx=0, 2\n",
            "idx=1, 9\n",
            "idx=0, 2\n",
            "idx=1, 9\n",
            "idx=0, 2\n",
            "idx=1, 9\n",
            "idx=0, 3\n",
            "idx=1, 10\n",
            "idx=0, 3\n",
            "idx=1, 10\n",
            "idx=0, 3\n",
            "idx=1, 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test 2d arry index: one row, one thread"
      ],
      "metadata": {
        "id": "syUf10J2I4pC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile a2.py\n",
        "#!python \n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "from pycuda import gpuarray\n",
        "import numpy as np\n",
        "\n",
        "mod = SourceModule(\"\"\"\n",
        "    #include <stdio.h>\n",
        "\n",
        "    __global__ void say_hi(int *out_gpu)\n",
        "    { \n",
        "      int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "      int idy = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "\n",
        "      printf(\"I am threadIdx:[%d][%d]  of block:[%d][%d] block size:[%d][%d][%d]  grid size:[%d][%d]  idx=%d, idy=%d\\\\n\", \\\n",
        "      threadIdx.x, threadIdx.y, blockIdx.x,blockIdx.y,blockDim.x,blockDim.y,blockDim.z, gridDim.x,gridDim.y,idx,idy);\n",
        "      for(int i=0;i<3;i++){\n",
        "          printf(\"idx=%d, %d\\\\n\",idx,out_gpu[idx*3+i]);\n",
        "          //printf(\"%d\\\\n\",out_gpu[idx]);\n",
        "      }\n",
        "      //__syncthreads();\n",
        "  \n",
        "    }\n",
        "    \"\"\")\n",
        "\n",
        "func = mod.get_function(\"say_hi\")\n",
        "\n",
        "\n",
        "data=np.array([21,2,3,38,9,10], dtype=np.int32)\n",
        "data=data.reshape(2,3)\n",
        "print('shape:',data.shape,data)\n",
        "data_gpu = gpuarray.to_gpu(data)\n",
        "func(data_gpu,block=(2,1,1),grid=(1,1,1))\n",
        "#print('modify pointer array:',data_gpu.get())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hK9KLy3OYOB6",
        "outputId": "c4c9b500-0a14-48f8-c8ae-b43038efa42f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting a2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python a2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5RLdC16YxRA",
        "outputId": "25bd02b2-536b-4a9c-c228-3c9f74ae708d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (2, 3) [[21  2  3]\n",
            " [38  9 10]]\n",
            "I am threadIdx:[0][0]  of block:[0][0] block size:[2][1][1]  grid size:[1][1]  idx=0, idy=0\n",
            "I am threadIdx:[1][0]  of block:[0][0] block size:[2][1][1]  grid size:[1][1]  idx=1, idy=0\n",
            "idx=0, 21\n",
            "idx=1, 38\n",
            "idx=0, 2\n",
            "idx=1, 9\n",
            "idx=0, 3\n",
            "idx=1, 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "data=np.array([21,2,3,38,9,10], dtype=np.int32)\n",
        "print(data.reshape(2,3))\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jn4CbsMNxEHT",
        "outputId": "1a595ee6-3ce1-48f8-ecbb-28e3c314537c"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[21  2  3]\n",
            " [38  9 10]]\n",
            "[21  2  3 38  9 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run naive_prefix.py"
      ],
      "metadata": {
        "id": "VVzrqW9wKBZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pycuda.autoinit\n",
        "import pycuda.driver as drv\n",
        "import numpy as np\n",
        "from pycuda import gpuarray\n",
        "from pycuda.compiler import SourceModule\n",
        "from time import time\n",
        "# this is a naive parallel prefix-sum kernel that uses shared memory\n",
        "naive_ker = SourceModule(\"\"\"\n",
        "__global__ void naive_prefix(double *vec, double *out)\n",
        "{\n",
        "     __shared__ double sum_buf[1024];     \n",
        "     int tid = threadIdx.x;     \n",
        "     sum_buf[tid] = vec[tid];\n",
        "     \n",
        "     // begin parallel prefix sum algorithm\n",
        "     \n",
        "     int iter = 1;\n",
        "     for (int i=0; i < 10; i++)\n",
        "     {\n",
        "         __syncthreads();\n",
        "         if (tid >= iter )\n",
        "         {\n",
        "             sum_buf[tid] = sum_buf[tid] + sum_buf[tid - iter];            \n",
        "         }\n",
        "         \n",
        "         iter *= 2;\n",
        "     }\n",
        "         \n",
        "    __syncthreads();\n",
        "    out[tid] = sum_buf[tid];\n",
        "    __syncthreads();\n",
        "        \n",
        "}\n",
        "\"\"\")\n",
        "naive_gpu = naive_ker.get_function(\"naive_prefix\")\n",
        "    \n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    \n",
        "    testvec = np.random.randn(1024).astype(np.float64)\n",
        "    testvec_gpu = gpuarray.to_gpu(testvec)\n",
        "    \n",
        "    outvec_gpu = gpuarray.empty_like(testvec_gpu)\n",
        "\n",
        "    naive_gpu( testvec_gpu , outvec_gpu, block=(1024,1,1), grid=(1,1,1))\n",
        "    \n",
        "    total_sum = sum( testvec)\n",
        "    total_sum_gpu = outvec_gpu[-1].get()\n",
        "    \n",
        "    print('outvec_gpu:',outvec_gpu)\n",
        "    print(\"Does our kernel work correctly? : {}\".format(np.allclose(total_sum_gpu , total_sum) ))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2B8sQV2H6Ns",
        "outputId": "51e30bbd-fc65-4d78-d5ee-bcd074bf7bb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "outvec_gpu: [ -1.20500879  -1.6132585   -2.71519557 ... -57.63849801 -58.57298875\n",
            " -59.34142221]\n",
            "Does our kernel work correctly? : True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### hello world\n",
        "https://documen.tician.de/pycuda/tutorial.html"
      ],
      "metadata": {
        "id": "iJFY8s_TbvAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPrAk7z1b4Fv",
        "outputId": "361f794d-9856-4538-e974-d782ea423c6c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0 0]\n",
            " [1 1 0]\n",
            " [0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### print input"
      ],
      "metadata": {
        "id": "cinZJ7SOOoaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile i.py\n",
        "#!python \n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "from pycuda import gpuarray\n",
        "import numpy as np\n",
        "\n",
        "mod = SourceModule(\"\"\"\n",
        "    #include <stdio.h>\n",
        "\n",
        "    __global__ void say_hi(int *out_gpu)\n",
        "    { \n",
        "      int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "     \n",
        "\n",
        "      printf(\"index: threadIdx.x =%d blockIdx.x =%d blockDim.x %d  \\\\n\", threadIdx.x , blockIdx.x , blockDim.x);\n",
        "      printf(\"thread id: [%d]  intput out_gpu=%d \\\\n\",idx,out_gpu[idx]);\n",
        "      \n",
        "      out_gpu[idx] *= 2;\n",
        "      printf(\"thread id 2: [%d]  out_gpu=%d \\\\n\",idx,out_gpu[idx]);\n",
        "\n",
        "    }\n",
        "    \"\"\")\n",
        "\n",
        "func = mod.get_function(\"say_hi\")\n",
        "\n",
        "\n",
        "t= np.array([1,2,3,4], dtype=np.int32)\n",
        "print('shape:',t.shape,t.size)\n",
        "t_gpu = gpuarray.to_gpu(t)\n",
        "func(t_gpu,block=(t.size,1,1),grid=(1,1,1))\n",
        "print(t_gpu.get())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhN00rk1OlsB",
        "outputId": "41439a09-ea34-4d39-b9b5-b6b48e7ba7d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting i.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python i.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34rSfTS-PILE",
        "outputId": "f33576f1-00cd-48ec-cdf3-f2b28529e5d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (4,) 4\n",
            "index: threadIdx.x =0 blockIdx.x =0 blockDim.x 4  \n",
            "index: threadIdx.x =1 blockIdx.x =0 blockDim.x 4  \n",
            "index: threadIdx.x =2 blockIdx.x =0 blockDim.x 4  \n",
            "index: threadIdx.x =3 blockIdx.x =0 blockDim.x 4  \n",
            "thread id: [0]  intput out_gpu=1 \n",
            "thread id: [1]  intput out_gpu=2 \n",
            "thread id: [2]  intput out_gpu=3 \n",
            "thread id: [3]  intput out_gpu=4 \n",
            "thread id 2: [0]  out_gpu=2 \n",
            "thread id 2: [1]  out_gpu=4 \n",
            "thread id 2: [2]  out_gpu=6 \n",
            "thread id 2: [3]  out_gpu=8 \n",
            "[2 4 6 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pass string to kernel"
      ],
      "metadata": {
        "id": "J7vgFyC2LOG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "s= np.array(['abc','hello,world'], dtype=object)\n",
        "print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4kfdzClLkuI",
        "outputId": "9cdc590d-5302-488e-b77a-39fa1a16e9b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['abc' 'hello,world']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile s.py\n",
        "#!python \n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "from pycuda import gpuarray\n",
        "import numpy as np\n",
        "\n",
        "mod = SourceModule(\"\"\"\n",
        "    #include <stdio.h>\n",
        "\n",
        "    __global__ void say_hi(int *out_gpu)\n",
        "    { \n",
        "      int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "      printf(\"thread id: [%d]  %d\\\\n\",idx,out_gpu);\n",
        "      //print c-str\n",
        "      for (int i=0;i<5;i++){\n",
        "        printf(\"%c\",out_gpu[0]);\n",
        "      }\n",
        "      printf(\" >end\\\\n\");\n",
        "\n",
        "    }\n",
        "    \"\"\")\n",
        "\n",
        "func = mod.get_function(\"say_hi\")\n",
        "\n",
        "\n",
        "s= np.array(['abcdf'], dtype=object)\n",
        "print('shape:',s.shape)\n",
        "print(s,s.data)\n",
        "s_gpu = gpuarray.to_gpu(s)\n",
        "func(s_gpu,block=(s.size,1,1),grid=(1,1,1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQy3qUsQLSDv",
        "outputId": "d95b3e4f-1eae-44df-e438-e3fce109bde2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting s.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python s.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3fsxgNvM8Rq",
        "outputId": "e7ae4cc4-16ec-48e3-f643-af5c3c6867dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/s.py:8: UserWarning: The CUDA compiler succeeded, but said the following:\n",
            "kernel.cu(8): warning #181-D: argument is incompatible with corresponding format string conversion\n",
            "\n",
            "\n",
            "  mod = SourceModule(\"\"\"\n",
            "shape: (1,)\n",
            "['abcdf'] <memory at 0x7f66026f9640>\n",
            "thread id: [0]  0\n",
            "����� >end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test tutaorl 2 https://cuda-tutorial.readthedocs.io/en/latest/tutorials/tutorial02/"
      ],
      "metadata": {
        "id": "ZcZfCqwBJIXR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZBYr6BUWNuLe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f620bb83-2192-46aa-d2dc-b6eb0d5e32a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1575  100  1575    0     0  47727      0 --:--:-- --:--:-- --:--:-- 47727\n",
            "total 8\n",
            "drwxr-xr-x 1 root root 4096 Mar 15 13:40 sample_data\n",
            "-rw-r--r-- 1 root root 1575 Mar 17 01:21 vector_add_thread.cu\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://cuda-tutorial.readthedocs.io/en/latest/tutorials/tutorial02/solutions/vector_add_thread.cu\n",
        "!ls -l"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc vector_add_thread.cu -o vector_add_grid\n",
        "!nvprof ./vector_add_grid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CXpoXFpKGi1",
        "outputId": "3e86dc57-587d-43c3-8aba-882082a30a10"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==2277== NVPROF is profiling process 2277, command: ./vector_add_grid\n",
            "PASSED\n",
            "==2277== Profiling application: ./vector_add_grid\n",
            "==2277== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   40.01%  26.437ms         1  26.437ms  26.437ms  26.437ms  [CUDA memcpy DtoH]\n",
            "                   34.70%  22.929ms         1  22.929ms  22.929ms  22.929ms  vector_add(float*, float*, float*, int)\n",
            "                   25.29%  16.712ms         2  8.3562ms  8.0535ms  8.6589ms  [CUDA memcpy HtoD]\n",
            "      API calls:   82.24%  328.28ms         3  109.43ms  115.35us  328.02ms  cudaMalloc\n",
            "                   16.88%  67.366ms         3  22.455ms  8.2867ms  50.200ms  cudaMemcpy\n",
            "                    0.61%  2.4278ms         3  809.27us  240.07us  1.1046ms  cudaFree\n",
            "                    0.23%  907.08us         1  907.08us  907.08us  907.08us  cuDeviceGetPCIBusId\n",
            "                    0.03%  129.28us       101  1.2790us     130ns  55.520us  cuDeviceGetAttribute\n",
            "                    0.01%  47.526us         1  47.526us  47.526us  47.526us  cudaLaunchKernel\n",
            "                    0.01%  25.889us         1  25.889us  25.889us  25.889us  cuDeviceGetName\n",
            "                    0.00%  1.8090us         3     603ns     199ns  1.3890us  cuDeviceGetCount\n",
            "                    0.00%  1.3160us         2     658ns     165ns  1.1510us  cuDeviceGet\n",
            "                    0.00%     529ns         1     529ns     529ns     529ns  cuModuleGetLoadingMode\n",
            "                    0.00%     288ns         1     288ns     288ns     288ns  cuDeviceTotalMem\n",
            "                    0.00%     214ns         1     214ns     214ns     214ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}