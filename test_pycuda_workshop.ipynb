{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonystz/gitpod/blob/main/test_pycuda_workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2JZt1GL5D6W"
      },
      "source": [
        "# Introduction to CUDA and PyCUDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FA_YN7HlGRP5"
      },
      "outputs": [],
      "source": [
        "!pip install pycuda # install cuda\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "O45Aq6Bzzcc-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95ae2606-c99b-4f88-871d-f0699a0acc59"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Mar 13 08:27:30 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   66C    P0    30W /  70W |    103MiB / 15360MiB |      1%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pycuda\n",
        "import pycuda.driver as drv\n",
        "drv.init()\n",
        "\n",
        "print('CUDA device query (PyCUDA version) \\n')\n",
        "\n",
        "print('Detected {} CUDA Capable device(s) \\n'.format(drv.Device.count()))\n",
        "\n",
        "for i in range(drv.Device.count()):\n",
        "    \n",
        "    gpu_device = drv.Device(i)\n",
        "    print('Device {}: {}'.format( i, gpu_device.name() )) \n",
        "    compute_capability = float( '%d.%d' % gpu_device.compute_capability() )\n",
        "    print('\\t Compute Capability: {}'.format(compute_capability))\n",
        "    print('\\t Total Memory: {} megabytes'.format(gpu_device.total_memory()//(1024**2)))\n",
        "\n",
        "\n",
        "    # The following will give us all remaining device attributes as seen \n",
        "    # in the original deviceQuery.\n",
        "    # We set up a dictionary as such so that we can easily index\n",
        "    # the values using a string descriptor.\n",
        "    \n",
        "    device_attributes_tuples = iter(gpu_device.get_attributes().items()) \n",
        "    device_attributes = {}\n",
        "\n",
        "        \n",
        "    for k, v in device_attributes_tuples:\n",
        "        device_attributes[str(k)] = v\n",
        "        # print(f'{k}->{v}')\n",
        "    # continue\n",
        "    num_mp = device_attributes['MULTIPROCESSOR_COUNT']\n",
        "    \n",
        "    # Cores per multiprocessor is not reported by the GPU!  \n",
        "    # We must use a lookup table based on compute capability.\n",
        "    # See the following:\n",
        "    # http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities\n",
        "    \n",
        "    cuda_cores_per_mp = { 5.0 : 128, 5.1 : 128, 5.2 : 128, 6.0 : 64, 6.1 : 128, 7.5 : 128}[compute_capability]\n",
        "    \n",
        "    print('\\t ({}) Multiprocessors, ({}) CUDA Cores / Multiprocessor: {} CUDA Cores'.format(num_mp, cuda_cores_per_mp, num_mp*cuda_cores_per_mp))\n",
        "    \n",
        "    device_attributes.pop('MULTIPROCESSOR_COUNT')\n",
        "    \n",
        "    for k in list(device_attributes.keys()):\n",
        "        print('\\t {}: {}'.format(k, device_attributes[k]))\n"
      ],
      "metadata": {
        "id": "CkYg1h5iF-x6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-g92eSBn5FlC"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "\n",
        "a = numpy.array([[1,1,1,1],[1,1,1,1],[1,1,1,1],[1,1,1,1]], dtype=numpy.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92L9E7WkJhsy"
      },
      "outputs": [],
      "source": [
        "a_gpu = cuda.mem_alloc(a.nbytes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1xaAt_NJjSb"
      },
      "outputs": [],
      "source": [
        "cuda.memcpy_htod(a_gpu, a)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSAkS6e2Jk38"
      },
      "outputs": [],
      "source": [
        "mod = SourceModule(\"\"\"\n",
        "  __global__ void doublify(float *a)\n",
        "  {\n",
        "    int idx = threadIdx.x + threadIdx.y*4;\n",
        "    a[idx] *= 2;\n",
        "    printf(\"loca is %zu, %f\\\\n\", idx,a[idx]);\n",
        "  }\n",
        "  \"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##pycuda index test"
      ],
      "metadata": {
        "id": "nMvWV6GeJY1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile a.py\n",
        "#!python \n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "from pycuda import gpuarray\n",
        "import numpy as np\n",
        "\n",
        "mod = SourceModule(\"\"\"\n",
        "    #include <stdio.h>\n",
        "\n",
        "    __global__ void say_hi(int *g_pcnt)\n",
        "    { \n",
        "      int l_count = 0;\n",
        "      __shared__ int g_count;\n",
        "      l_count +=1;\n",
        "\n",
        "      __syncthreads();\n",
        "      g_count +=1;\n",
        "\n",
        "      *g_pcnt +=22;\n",
        "\n",
        "      printf(\"I am threadIdx:[%d[%d]  of block:[%d][%d] block size:[%d][%d][%d]  grid size:[%d][%d]:\\\\n\", threadIdx.x, threadIdx.y, blockIdx.x,blockIdx.y,blockDim.x,blockDim.y,blockDim.z, gridDim.x,gridDim.y);\n",
        "      \n",
        "      int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "      int idy = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "      printf(\"thread id: [%d][%d]   -> l_count=%d ,g_count=%d\\\\n\",idx,idy,l_count,g_count);\n",
        "    }\n",
        "    \"\"\")\n",
        "\n",
        "func = mod.get_function(\"say_hi\")\n",
        "acnt= np.array([0],dtype=np.int32)\n",
        "print('original pointer array:',acnt)\n",
        "\n",
        "acnt_gpu=gpuarray.to_gpu(acnt)\n",
        "func(acnt_gpu,block=(3,2,1),grid=(2,1,1))\n",
        "print('modify pointer array:',acnt_gpu.get())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78Ed3tu7G2qf",
        "outputId": "be3542eb-bece-41ba-bf2d-bf409725c587"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting a.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python a.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUCbHjV7IwkW",
        "outputId": "db0acfd6-be13-48e4-b41d-906d7fbb5589"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original pointer array: [0]\n",
            "I am threadIdx:[0[0]  of block:[1][0] block size:[3][2][1]  grid size:[2][1]:\n",
            "I am threadIdx:[1[0]  of block:[1][0] block size:[3][2][1]  grid size:[2][1]:\n",
            "I am threadIdx:[2[0]  of block:[1][0] block size:[3][2][1]  grid size:[2][1]:\n",
            "I am threadIdx:[0[1]  of block:[1][0] block size:[3][2][1]  grid size:[2][1]:\n",
            "I am threadIdx:[1[1]  of block:[1][0] block size:[3][2][1]  grid size:[2][1]:\n",
            "I am threadIdx:[2[1]  of block:[1][0] block size:[3][2][1]  grid size:[2][1]:\n",
            "I am threadIdx:[0[0]  of block:[0][0] block size:[3][2][1]  grid size:[2][1]:\n",
            "I am threadIdx:[1[0]  of block:[0][0] block size:[3][2][1]  grid size:[2][1]:\n",
            "I am threadIdx:[2[0]  of block:[0][0] block size:[3][2][1]  grid size:[2][1]:\n",
            "I am threadIdx:[0[1]  of block:[0][0] block size:[3][2][1]  grid size:[2][1]:\n",
            "I am threadIdx:[1[1]  of block:[0][0] block size:[3][2][1]  grid size:[2][1]:\n",
            "I am threadIdx:[2[1]  of block:[0][0] block size:[3][2][1]  grid size:[2][1]:\n",
            "thread id: [3][0]   -> l_count=1 ,g_count=0\n",
            "thread id: [4][0]   -> l_count=1 ,g_count=0\n",
            "thread id: [5][0]   -> l_count=1 ,g_count=0\n",
            "thread id: [3][1]   -> l_count=1 ,g_count=0\n",
            "thread id: [4][1]   -> l_count=1 ,g_count=0\n",
            "thread id: [5][1]   -> l_count=1 ,g_count=0\n",
            "thread id: [0][0]   -> l_count=1 ,g_count=0\n",
            "thread id: [1][0]   -> l_count=1 ,g_count=0\n",
            "thread id: [2][0]   -> l_count=1 ,g_count=0\n",
            "thread id: [0][1]   -> l_count=1 ,g_count=0\n",
            "thread id: [1][1]   -> l_count=1 ,g_count=0\n",
            "thread id: [2][1]   -> l_count=1 ,g_count=0\n",
            "modify pointer array: [22]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "\n",
        "void helloCPU()\n",
        "{\n",
        "  printf(\"Hello from the CPU.\\n\");\n",
        "}\n",
        "\n",
        "/*\n",
        " * The addition of `__global__` signifies that this function\n",
        " * should be launced on the GPU.\n",
        " */\n",
        "\n",
        "__global__ void helloGPU()\n",
        "{\n",
        "  printf(\"Hello from the GPU.\\n\");\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "   helloCPU();\n",
        "\n",
        "\n",
        "  /*\n",
        "   * Add an execution configuration with the <<<...>>> syntax\n",
        "   * will launch this function as a kernel on the GPU.\n",
        "   */\n",
        "\n",
        "  helloGPU<<<1, 1>>>();\n",
        "\n",
        "  /*\n",
        "   * `cudaDeviceSynchronize` will block the CPU stream until\n",
        "   * all GPU kernels have completed.\n",
        "   */\n",
        "\n",
        "  cudaDeviceSynchronize();\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gT21dGyEHSWP",
        "outputId": "f4b31fdc-b65f-4cb9-d4b2-3be49d5faf6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Cell magic `%%cu` not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuPVBDIGEDk8",
        "outputId": "cc860c02-1652-4f1e-d2ff-c326585cedcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBQ0blkNJnIg",
        "outputId": "8aebce4c-0368-4e96-ddfc-7b43fc86741c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "host: (4, 4)\n",
            "     0    1    2    3\n",
            "0  1.0  1.0  1.0  1.0\n",
            "1  1.0  1.0  1.0  1.0\n",
            "2  1.0  1.0  1.0  1.0\n",
            "3  1.0  1.0  1.0  1.0\n",
            "     0    1    2    3\n",
            "0  1.0  1.0  1.0  1.0\n",
            "1  1.0  1.0  1.0  1.0\n",
            "2  1.0  1.0  1.0  1.0\n",
            "3  1.0  1.0  1.0  1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/google/colab/_variable_inspector.py:27: UserWarning: device_allocation in out-of-thread context could not be cleaned up\n",
            "  globals().clear()\n"
          ]
        }
      ],
      "source": [
        "func = mod.get_function(\"doublify\")\n",
        "print('host:',a.shape)\n",
        "from pycuda import gpuarray\n",
        "import pandas as pd\n",
        "print(pd.DataFrame(a))\n",
        "a_on_gpu= gpuarray.to_gpu(a)\n",
        "func(a_gpu, block=(4,4,1))\n",
        "print(pd.DataFrame(a_on_gpu.get()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ef82NDPJqWV",
        "outputId": "fcfe1f93-8c5a-45a9-a57d-5939dbc3fb3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 4)\n",
            "[[-1.3746789  -1.6419895   3.3047218  -1.1505861 ]\n",
            " [ 2.1979356   1.8518921  -1.9868276  -1.7164422 ]\n",
            " [ 0.14977352  1.058711    0.2419031  -0.44884723]\n",
            " [-3.113357    0.11188176  0.32294306 -4.2692833 ]]\n",
            "[[-0.6873394  -0.82099473  1.6523609  -0.57529306]\n",
            " [ 1.0989678   0.92594606 -0.9934138  -0.8582211 ]\n",
            " [ 0.07488676  0.5293555   0.12095155 -0.22442362]\n",
            " [-1.5566785   0.05594088  0.16147153 -2.1346416 ]]\n"
          ]
        }
      ],
      "source": [
        "a_doubled = numpy.empty_like(a)\n",
        "cuda.memcpy_dtoh(a_doubled, a_gpu)\n",
        "print(a_doubled.shape)\n",
        "print(a_doubled)\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXx_p97mJs4Z"
      },
      "outputs": [],
      "source": [
        "b = numpy.random.randn(4,4)\n",
        "b = b.astype(numpy.float32)\n",
        "c = numpy.random.randn(4,4)\n",
        "c = c.astype(numpy.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQYD348qKgva"
      },
      "outputs": [],
      "source": [
        "mod2 = SourceModule(\"\"\"\n",
        "  __global__ void add2(float *a, float *b)\n",
        "  {\n",
        "    int idx = threadIdx.x + threadIdx.y*4;\n",
        "    a[idx] += b[idx];\n",
        "  }\n",
        "  \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vs_5Hb-6Kr-C"
      },
      "outputs": [],
      "source": [
        "b_gpu = cuda.mem_alloc(b.nbytes)\n",
        "c_gpu = cuda.mem_alloc(c.nbytes)\n",
        "\n",
        "cuda.memcpy_htod(b_gpu, b)\n",
        "cuda.memcpy_htod(c_gpu, c)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwA4tpOtLE5_"
      },
      "outputs": [],
      "source": [
        "func = mod2.get_function(\"add2\")\n",
        "func(b_gpu,c_gpu, block=(4,4,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zN8iBYDM_00"
      },
      "outputs": [],
      "source": [
        "added = numpy.empty_like(b)\n",
        "cuda.memcpy_dtoh(added, b_gpu)\n",
        "print(added)\n",
        "print(b)\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJBVoR8ANgx5"
      },
      "source": [
        "# Exercises\n",
        "\n",
        "1. Write a cuda kernel to find the elementwise square of a matrix\n",
        "2. Write a cuda kernel to find a matrix, which when added to the given matrix results in every element being equal to zero\n",
        "3. Write a cuda kernel to multiply two matrices:\n",
        "    1. Assume square matrices, with dimensions < 1024\n",
        "    2. Assume square matrices, with dimensions > 1024\n",
        "    3. Assume non-square matrices, with dimensions > 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBYr6BUWNuLe"
      },
      "outputs": [],
      "source": [
        "1. "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}